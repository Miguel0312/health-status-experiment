{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "amQ8VNFgmyOr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "#TODO: improve configuration\n",
    "NUMBER_OF_SAMPLES = 24\n",
    "# DATA_FILE = \"data/mini-baidu-dataset.csv\"\n",
    "# DATA_FILE = \"baidu-dataset.csv\"\n",
    "DATA_FILE = \"data/baidu-dataset.csv\"\n",
    "CHANGE_RATE_INTERVAL = 6\n",
    "FEATURE_COUNT = 2\n",
    "HEALTH_CLASSES_COUNT = 2\n",
    "\n",
    "class HealthStatusAlgorithm(Enum):\n",
    "\tLINEAR = 1\n",
    "\n",
    "class FeatureSelectionAlgorithm(Enum):\n",
    "\tZ_SCORE = 1\n",
    "\n",
    "def computeChangeRates(df, interval):\n",
    "\t# Remove the serial number\n",
    "\tratesColumns = list(df.columns)[2:]\n",
    "\ttitles = [column + \" Change Rate\" for column in ratesColumns]\n",
    "\tfor title in titles:\n",
    "\t\tdf[title] = [None for i in range(len(df))]\n",
    "\n",
    "\tfor idx, column in enumerate(ratesColumns):\n",
    "\t\ttmpValues = list(df[column])\n",
    "\t\ttmpValues = [np.nan for i in range(interval)] + tmpValues[:-interval]\n",
    "\t\tdif = np.subtract(df[column], tmpValues)\n",
    "\t\tdf[titles[idx]] = dif\n",
    "\n",
    "\tdf = df.dropna()\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def z_score(goodSamples, badSamples):\n",
    "\tnf = len(badSamples)\n",
    "\tng = len(goodSamples)\n",
    "\tmf = np.average(badSamples)\n",
    "\tmg = np.average(goodSamples)\n",
    "\tvf = np.var(badSamples)\n",
    "\tvg = np.var(goodSamples)\n",
    "\n",
    "\tif vf == 0 and vg == 0:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn math.fabs(mf-mg)/math.sqrt(vf/nf + vg/ng)\n",
    "\n",
    "def featureSelection(df, algorithm, toKeepCount):\n",
    "\tfunc = None\n",
    "\tmatch algorithm:\n",
    "\t\tcase FeatureSelectionAlgorithm.Z_SCORE:\n",
    "\t\t\tfunc = z_score\n",
    "\n",
    "\t# Remove the serial and status\n",
    "\tcolumns = list(df.columns)[2:]\n",
    "\tgood_hard_drives = df[df[\"Drive Status\"] == 1]\n",
    "\tbad_hard_drives = df[df[\"Drive Status\"] == -1]\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tfor col in columns:\n",
    "\t\tgoodSamples = list(good_hard_drives[col])\n",
    "\t\tbadSamples = list(bad_hard_drives[col])\n",
    "\n",
    "\t\tresults.append((func(goodSamples, badSamples), col))\n",
    "\n",
    "\tresults.sort()\n",
    "\ttoKeep = list(df.columns)[0:2] + [result[1] for result in results][:toKeepCount]\n",
    "\n",
    "\treturn df[toKeep]\n",
    "\n",
    "# Keeps only the last N samples of each disk on the dataframe\n",
    "def getLastSamples(df, N):\n",
    "\tserialNumbers = df[\"serial-number\"].unique()\n",
    "\ttoKeep = []\n",
    "\tfor serialNumber in serialNumbers:\n",
    "\t\tindices = df[df[\"serial-number\"] == serialNumber].index[-N:]\n",
    "\t\tfor index in indices:\n",
    "\t\t\ttoKeep.append(index)\n",
    "\n",
    "\treturn df.loc[toKeep]\n",
    "\n",
    "# Linearly map the values [0,n-1] to [maxi, mini]\n",
    "# TODO: add an option to decide if the bigger or smaller intervals get less elements\n",
    "# For now, there are more elements with bigger values\n",
    "def LinearAlgorithm(mini, maxi, i, n):\n",
    "\treturn maxi - math.floor((maxi-(mini-1))*i/n) - 1\n",
    "\n",
    "# A column with a score in [1,maxLevel] is given to each sample\n",
    "# If good is set, it is always equal to maxLevel\n",
    "# Else the algorithm is used to give a score in [1,maxLevel-1]\n",
    "def addHealthStatus(df, good, algorithm, maxLevel):\n",
    "\tif good:\n",
    "\t\tdf[\"Health Status\"] = [maxLevel - 1 for i in range(len(df))]\n",
    "\t\treturn df\n",
    "\n",
    "\tfunc = None\n",
    "\n",
    "\tmatch algorithm:\n",
    "\t\tcase HealthStatusAlgorithm.LINEAR:\n",
    "\t\t\tfunc = LinearAlgorithm\n",
    "\n",
    "\tserialNumbers = df[\"serial-number\"].unique()\n",
    "\thealthStatusValues = []\n",
    "\tfor serialNumber in serialNumbers:\n",
    "\t\tcnt = len(df[df[\"serial-number\"] == serialNumber])\n",
    "\t\tnewValues = [func(1, maxLevel-1, i, cnt) for i in range(cnt)]\n",
    "\t\thealthStatusValues = healthStatusValues + newValues\n",
    "\n",
    "\tdf[\"Health Status\"] = healthStatusValues\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "wu8JFdy1nsLJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Reading data file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing change rates\n",
      "Selecting 2 features using the Z_SCORE algorithm\n",
      "Features kept: ['Power on Hours Change Rate', 'Hardware ECC Recovered Change Rate']\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO,format='%(message)s')\n",
    "\n",
    "logger.info(\"Started\")\n",
    "\n",
    "logger.info(\"Reading data file\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data = pd.read_csv(DATA_FILE)\n",
    "\n",
    "logger.info(\"Computing change rates\")\n",
    "data = computeChangeRates(data, CHANGE_RATE_INTERVAL)\n",
    "\n",
    "good_hard_drives = data[data[\"Drive Status\"] == 1]\n",
    "bad_hard_drives = getLastSamples(data[data[\"Drive Status\"] == -1], NUMBER_OF_SAMPLES)\n",
    "\n",
    "data = pd.concat([bad_hard_drives, good_hard_drives])\n",
    "\n",
    "# TODO: check if the features change a lot when CHANGE_RATE_INTERVAL and NUMBER_OF_SAMPLES change\n",
    "logger.info(\"Selecting %d features using the %s algorithm\", FEATURE_COUNT, FeatureSelectionAlgorithm.Z_SCORE.name)\n",
    "data = featureSelection(data, FeatureSelectionAlgorithm.Z_SCORE, FEATURE_COUNT)\n",
    "logger.info(\"Features kept: %s\", str(list(data.columns)[2:]))\n",
    "\n",
    "good_hard_drives = data[data[\"Drive Status\"] == 1]\n",
    "bad_hard_drives = getLastSamples(data[data[\"Drive Status\"] == -1], NUMBER_OF_SAMPLES)\n",
    "\n",
    "bad_hard_drives = addHealthStatus(bad_hard_drives, False, HealthStatusAlgorithm.LINEAR, HEALTH_CLASSES_COUNT)\n",
    "good_hard_drives = addHealthStatus(good_hard_drives, True, HealthStatusAlgorithm.LINEAR, HEALTH_CLASSES_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfXsiw1ipejY",
    "outputId": "315b4daf-4996-485e-f306-1eeaca5776fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYLnjF7psU07",
    "outputId": "486ce29f-fd43-4a40-8c25-eb4992de1f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=2, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(FEATURE_COUNT, 30)\n",
    "        self.fc2 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "1CkvoINH9yFD"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # print(output)\n",
    "        # pred_probab = nn.LogSoftmax()(output)\n",
    "        # print(pred_probab)\n",
    "        # return -torch.sum(torch.tensor([math.log(pred_probab[i][target[i]-1]) for i in range(len(output))]))\n",
    "        return -torch.sum(torch.tensor([output[i][target[i]-1] for i in range(len(output))]))\n",
    "        return -torch.sum(torch.tensor([pred_probab[i][target[i]-1] for i in range(len(output))]))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "pNsE-VQQ-N-d"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X).squeeze()\n",
    "        loss = loss_fn(pred, y.float())\n",
    "\n",
    "        # Backpropagation\n",
    "        # loss.requires_grad = True\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.mean(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct_good, correct_bad = 0.0, 0, 0\n",
    "    size_good, size_bad = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss_fn(pred, y.float()).mean()\n",
    "            # correct += (pred.argmax(1) == y-1).type(torch.float).sum().item()\n",
    "            for i in range(len(X)):\n",
    "              if y[i] == HEALTH_CLASSES_COUNT:\n",
    "                size_good += 1\n",
    "                correct_good += 1 if pred[i] > 0 else 0\n",
    "              else:\n",
    "                size_bad += 1\n",
    "                correct_bad += 1 if pred[i] < 0 else 0\n",
    "    test_loss /= num_batches\n",
    "    correct_good /= size_good\n",
    "    correct_bad /= size_bad\n",
    "    # print(size_good)\n",
    "    # print(size_bad)\n",
    "    print(f\"Test Error: \\n FAR: {(100*(1-correct_good)):>0.1f}%, FDR: {(100*correct_bad):>0.1f}% Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "mLoTaf-o-_FC"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ratio = bad_hard_drives.size / good_hard_drives.size\n",
    "\n",
    "# train_good = good_hard_drives.sample(frac=0.8 * ratio)\n",
    "# test_good = good_hard_drives.sample(frac=0.2 * ratio)\n",
    "\n",
    "# train_bad = bad_hard_drives.sample(frac=0.8)\n",
    "# test_bad = bad_hard_drives.sample(frac=0.2)\n",
    "\n",
    "# training_set = pd.concat([train_good, train_bad])\n",
    "# test_set = pd.concat([test_good, test_bad])\n",
    "\n",
    "# # training_set = train_good\n",
    "# # test_set = test_good\n",
    "\n",
    "# training_set = training_set.drop([\"serial-number\", \"Drive Status\"] , axis = 1)\n",
    "# train_tensor = torch.tensor(training_set.drop(\"Health Status\", axis = 1).values, dtype=torch.float)\n",
    "\n",
    "# train_dataset = TensorDataset(train_tensor, torch.IntTensor(training_set[\"Health Status\"].to_list()))\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# test_set = test_set.drop([\"serial-number\", \"Drive Status\"] , axis = 1)\n",
    "# test_tensor = torch.tensor(test_set.drop(\"Health Status\", axis = 1).values, dtype=torch.float)\n",
    "\n",
    "# test_dataset = TensorDataset(test_tensor, torch.IntTensor(test_set[\"Health Status\"].to_list()))\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqjnJa0K-WOz",
    "outputId": "a4cf17da-eb6d-4425-ed20-521cc278bff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x772e6f9f6af0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1) # This returns that all HDs are failing\n",
    "# torch.manual_seed(1) # This returns that all HDs are working\n",
    "\n",
    "# epochs = 200\n",
    "# model = NeuralNetwork().to(device)\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = bad_hard_drives.size / good_hard_drives.size\n",
    "\n",
    "good_hard_drives = good_hard_drives.sample(frac=ratio)\n",
    "\n",
    "df = pd.concat([good_hard_drives, bad_hard_drives])\n",
    "\n",
    "df = df.drop([\"serial-number\", \"Drive Status\"] , axis = 1)\n",
    "\n",
    "y=df.pop(\"Health Status\")\n",
    "X=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x_train= Variable(torch.from_numpy(np.array(x_train)).type(torch.FloatTensor))\n",
    "x_test= Variable(torch.from_numpy(np.array(x_test)).type(torch.FloatTensor))\n",
    "y_train= Variable(torch.from_numpy(np.array(y_train)).type(torch.LongTensor))\n",
    "y_test= Variable(torch.from_numpy(np.array(y_test)).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 0.69418156\n",
      "epoch: 40, loss = 0.69418156\n",
      "epoch: 60, loss = 0.69418156\n",
      "epoch: 80, loss = 0.69418156\n",
      "epoch: 100, loss = 0.69418156\n",
      "epoch: 120, loss = 0.69418156\n",
      "epoch: 140, loss = 0.69418156\n",
      "epoch: 160, loss = 0.69418156\n",
      "epoch: 180, loss = 0.69418156\n",
      "epoch: 200, loss = 0.69418156\n",
      "epoch: 220, loss = 0.69418156\n",
      "epoch: 240, loss = 0.69418156\n",
      "epoch: 260, loss = 0.69418156\n",
      "epoch: 280, loss = 0.69418156\n",
      "epoch: 300, loss = 0.69418156\n",
      "epoch: 320, loss = 0.69418156\n",
      "epoch: 340, loss = 0.69418156\n",
      "epoch: 360, loss = 0.69418156\n",
      "epoch: 380, loss = 0.69418156\n",
      "epoch: 400, loss = 0.69418156\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x_train).squeeze()\n",
    "    loss = loss_fn(y_pred, y_train.float())             \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.mean().item():.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bqCyaawOGHKI",
    "outputId": "749d996f-399a-43fa-c359-c9ef462c7fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5223],\n",
      "        [0.5221],\n",
      "        [0.5216],\n",
      "        ...,\n",
      "        [0.5221],\n",
      "        [0.5208],\n",
      "        [0.5219]])\n",
      "FAR: 0.0%, FDR: 0.0%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_predicted = model(x_test)\n",
    "    print(y_predicted)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    cnt = [0, 0]\n",
    "    far, fdr = 0, 0\n",
    "    for i in range(len(y_predicted)):\n",
    "        cnt[y_test[i]] += 1\n",
    "        if(y_predicted_cls[i] == y_test[i]):\n",
    "            if y_test[i] == 0:\n",
    "                fdr += 1\n",
    "            else:\n",
    "                far += 1\n",
    "    print(f'FAR: {100*(cnt[1]-far)/cnt[1]}%, FDR: {100*fdr/cnt[0]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
